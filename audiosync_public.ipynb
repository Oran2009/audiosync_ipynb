{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "#Description: Displays Waveplot\n",
    "#Input: File Location, numpy.ndarray (audio), sampling rate\n",
    "#Return: None\n",
    "#Uses: Librosa, Matplotlib\n",
    "def displayWaveplot(file, audio, sr):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.title('Waveplot for {}'.format(file))\n",
    "    librosa.display.waveplot(audio, sr=sr)\n",
    "    plt.tight_layout()\n",
    "#Description: Displays Spectrogram\n",
    "#Input: File Location, numpy.ndarray (audio), sampling rate\n",
    "#Return: None\n",
    "#Uses: Librosa, Matplotlib\n",
    "def displaySpectrogram(file, audio, sr):\n",
    "    x = librosa.stft(audio)\n",
    "    db = librosa.amplitude_to_db(abs(x))\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.title('Spectrogram for {}'.format(file))\n",
    "    librosa.display.specshow(db, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "#Description: Displays Chromagram\n",
    "#Input: File Location, numpy.ndarray (audio), sampling rate\n",
    "#Return: None\n",
    "#Uses: Librosa, Matplotlib\n",
    "def displayChromagram(file, audio, sr):\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr, tuning=0, norm=2,\n",
    "                                             hop_length=hop_size, n_fft=n_fft)\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Chroma Representation of {}'.format(file))\n",
    "    librosa.display.specshow(chroma, x_axis='time',\n",
    "                             y_axis='chroma', cmap='gray_r', hop_length=hop_size)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "#Description: Displays Warping Path\n",
    "#Input: video path, audio path, sampling rate\n",
    "#Return: None\n",
    "#Uses: Librosa, Matplotlib   \n",
    "def displayWarpingPath(video, audio, sr, hop_size):\n",
    "    video_chroma = defineChromagram(video, sr)\n",
    "    audio_chroma = defineChromagram(audio, sr)\n",
    "    #Align\n",
    "    D, wp = librosa.sequence.dtw(X=video_chroma, Y=audio_chroma, metric='cosine')\n",
    "\n",
    "    fig, (ax_video, ax_audio) = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "    # Plot x_1\n",
    "    librosa.display.waveplot(video, sr=sr, x_axis='time', ax=ax_video)\n",
    "    ax_video.set(title='Video Waveplot')\n",
    "\n",
    "    # Plot x_2\n",
    "    librosa.display.waveplot(audio, sr=sr, x_axis='time', ax=ax_audio)\n",
    "    ax_audio.set(title='Audio Waveplot')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    trans_figure = fig.transFigure.inverted()\n",
    "    lines2 = []\n",
    "    arrows = 100\n",
    "    points_idx = np.int16(np.round(np.linspace(0, wp.shape[0] - 1, arrows)))\n",
    "\n",
    "    audio_times = []\n",
    "    video_times = []\n",
    "    # for tp1, tp2 in zip((wp[points_idx, 0]) * hop_size, (wp[points_idx, 1]) * hop_size):\n",
    "    for tp1, tp2 in wp[points_idx] * hop_size / sr:\n",
    "        # get position on axis for a given index-pair\n",
    "        coord1 = trans_figure.transform(ax_video.transData.transform([tp1, 0]))\n",
    "        coord2 = trans_figure.transform(ax_audio.transData.transform([tp2, 0]))\n",
    "        audio_times.append(coord1[0])\n",
    "        video_times.append(coord2[0])\n",
    "\n",
    "        # draw a line\n",
    "        line = matplotlib.lines.Line2D((coord1[0], coord2[0]),\n",
    "                                       (coord1[1], coord2[1]),\n",
    "                                       transform=fig.transFigure,\n",
    "                                       color='r')\n",
    "        lines2.append(line)\n",
    "\n",
    "    fig.lines = lines2\n",
    "    plt.tight_layout()\n",
    "#Description: Computes a chromagram\n",
    "#Input: numpy.ndarray (audio), sampling rate\n",
    "#Return: numpy.ndarray (Normalized energy for each chroma bin at each frame)\n",
    "#Uses: Librosa\n",
    "def defineChromagram(audio, sr):\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr, tuning=0, norm=2,\n",
    "                                             hop_length=hop_size, n_fft=n_fft)\n",
    "    return chroma\n",
    "#Description: Runs .bat file to combine video and audio\n",
    "#Input: Location of audio file, Location of Video File, Save Location\n",
    "#Return: None\n",
    "#Uses: ffmpeg\n",
    "def combine(audio_file, video_file, save_location):\n",
    "    cmd = ['ffmpeg','-y','-i',video_file,'-i',audio_file,'-map', '0:v','-map', '1:a','-c:v','copy','-c:a','aac','-b:a','160k',save_location]\n",
    "    subprocess.run(cmd)\n",
    "# Description: Runs .bat file to extract audio file from video\n",
    "# Input: Location of Video File, Save Location\n",
    "#Return: None\n",
    "#Uses: ffmpeg\n",
    "def extract(video_file, save_location):\n",
    "    cmd = ['ffmpeg', '-y', '-loglevel', 'quiet', '-i', video_file, save_location]\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "# Vars\n",
    "n_fft = 4410 # ftt window size\n",
    "hop_size = 512 # hop length\n",
    "sampling_rate = 44100 # sampling rate\n",
    "duration_limit = 120 # maximum duration of audio-video clips used to synchronize in seconds\n",
    "\n",
    "# CHANGE THESE!\n",
    "audio_file = 'PATH TO AUDIO FILE'\n",
    "video_file = 'PATH TO VIDEO FILE'\n",
    "video_audio_file = 'SAME AS video_file but .wav'\n",
    "synced_audio = 'PATH TO SAVE SYNCED AUDIO . . . ex. synced_audio.wav'\n",
    "save_file = 'PATH TO OUTPUT SYNCED AND COMBINED .MP4 FILE'\n",
    "\n",
    "##############---------LOAD INPUT---------##############\n",
    "\n",
    "# Load audio file\n",
    "audio, _ = librosa.load(audio_file, sr=sampling_rate, mono=True, duration=duration_limit)\n",
    "\n",
    "# Load video file, creates .wav file of the video audio\n",
    "handle, video_audio_file = tempfile.mkstemp(suffix='.wav')\n",
    "os.close(handle)\n",
    "extract(video_file, video_audio_file)\n",
    "video, _ = librosa.load(video_audio_file, sr=sampling_rate, mono=True, duration=duration_limit)\n",
    "os.unlink(video_audio_file)\n",
    "\n",
    "##############---------FEATURE EXTRACTION---------##############\n",
    "\n",
    "# Feature Extraction for Audio\n",
    "audio_chroma = defineChromagram(audio, sampling_rate)\n",
    "\n",
    "# Feature Extraction for Video\n",
    "video_chroma = defineChromagram(video, sampling_rate)\n",
    "\n",
    "##############---------RQA---------##############\n",
    "\n",
    "# Performs subsequence DTW\n",
    "xsim = librosa.segment.cross_similarity(audio_chroma, video_chroma, mode='affinity', metric='cosine')\n",
    "L_score, L_path = librosa.sequence.rqa(xsim, np.inf, np.inf, backtrack=True)\n",
    "\n",
    "audio_times = []\n",
    "video_times = []\n",
    "diff_times = []\n",
    "for v,a in L_path * hop_size / sampling_rate:\n",
    "    A = float(a)\n",
    "    V = float(v)\n",
    "    audio_times.append(A)\n",
    "    video_times.append(V)\n",
    "    diff_times.append((A-V))    \n",
    "    \n",
    "##############---------PLOTS---------##############\n",
    "displayWaveplot(video_audio_file, video, sampling_rate)\n",
    "displaySpectrogram(video_audio_file, video, sampling_rate)\n",
    "displayChromagram(video_audio_file, video, sampling_rate)\n",
    "displayWaveplot(audio_file, audio, sampling_rate)\n",
    "displaySpectrogram(audio_file, audio, sampling_rate)\n",
    "displayChromagram(audio_file, audio, sampling_rate)\n",
    "displayWarpingPath(video, audio, sampling_rate, hop_size)\n",
    "    \n",
    "##############---------SYNC PROCESS---------##############\n",
    "\n",
    "diff_times = np.array(diff_times)\n",
    "# Find mean of time differences\n",
    "mean = np.average(diff_times)\n",
    "std = np.std(diff_times)\n",
    "diff_times = [d for d in diff_times if np.abs(d-mean)<(0.5*std)]\n",
    "diff = np.average(diff_times)\n",
    "\n",
    "# Setting move option\n",
    "move = False # True = move audio left...False = move audio right\n",
    "if (diff > 0): move = True\n",
    "else: move = False\n",
    "    \n",
    "# Sync\n",
    "audio = AudioSegment.from_wav(audio_file)\n",
    "\n",
    "if (move):\n",
    "    # Trim diff seconds from beginning\n",
    "    final = audio[diff*1000:]\n",
    "else:\n",
    "    # Add diff seconds of silence to beginning\n",
    "    silence = AudioSegment.silent(duration=-diff*1000)\n",
    "    final = silence + audio\n",
    "\n",
    "# Export synced audio\n",
    "final.export(synced_audio, format='wav')\n",
    "\n",
    "##############---------COMBINE PROCESS---------##############\n",
    "combine(synced_audio, video_file, save_file)\n",
    "print(\"Successfully synced and combined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
